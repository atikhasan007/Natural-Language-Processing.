{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf92d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer,RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30315b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample text documents\n",
    "documents = [\n",
    "    'The cats are running and jumping in the garden.',\n",
    "    'She is a beautiful runner and loves to run fast.',\n",
    "    'Running help to build stamina and strength.',\n",
    "    'He ran swiftyly and caught the ball'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1947d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\") # multiple language support \n",
    "\n",
    "#define a simple regex fro stemming (this can be customized)\n",
    "regex_pattern = r'(ing|ed|es|s)$'\n",
    "regex_stemmer = RegexpStemmer(regex_pattern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3f29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to apply different semmers\n",
    "def apply_stemmers(documents):\n",
    "    results = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        #tokenize the document\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        \n",
    "        \n",
    "        #apply different stemmers \n",
    "        porter_stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "        lancaster_stems = [lancaster_stemmer.stem(token) for token in tokens]\n",
    "        snowball_stems = [snowball_stemmer.stem(token) for token in tokens]\n",
    "        regex_stems = [regex_stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        #store results \n",
    "        results[doc] = {\n",
    "            'porter' : porter_stems,\n",
    "            'lancaster':lancaster_stems,\n",
    "            'snowball':snowball_stems,\n",
    "            'regex':regex_stems\n",
    "        }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f17955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Documnets: The cats are running and jumping in the garden.\n",
      "Porter,stems:['the', 'cat', 'are', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "Lancaster,stems:['the', 'cat', 'ar', 'run', 'and', 'jump', 'in', 'the', 'gard', '.']\n",
      "Snowball,stems:['the', 'cat', 'are', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "Regex,stems:['the', 'cat', 'are', 'runn', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "\n",
      "Original Documnets: She is a beautiful runner and loves to run fast.\n",
      "Porter,stems:['she', 'is', 'a', 'beauti', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "Lancaster,stems:['she', 'is', 'a', 'beauty', 'run', 'and', 'lov', 'to', 'run', 'fast', '.']\n",
      "Snowball,stems:['she', 'is', 'a', 'beauti', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "Regex,stems:['she', 'i', 'a', 'beautiful', 'runner', 'and', 'lov', 'to', 'run', 'fast', '.']\n",
      "\n",
      "Original Documnets: Running help to build stamina and strength.\n",
      "Porter,stems:['run', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Lancaster,stems:['run', 'help', 'to', 'build', 'stamin', 'and', 'strength', '.']\n",
      "Snowball,stems:['run', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Regex,stems:['runn', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "\n",
      "Original Documnets: He ran swiftyly and caught the ball\n",
      "Porter,stems:['he', 'ran', 'swiftyli', 'and', 'caught', 'the', 'ball']\n",
      "Lancaster,stems:['he', 'ran', 'swifty', 'and', 'caught', 'the', 'bal']\n",
      "Snowball,stems:['he', 'ran', 'swiftyli', 'and', 'caught', 'the', 'ball']\n",
      "Regex,stems:['he', 'ran', 'swiftyly', 'and', 'caught', 'the', 'ball']\n"
     ]
    }
   ],
   "source": [
    "#apply the stemmers to the sample documents\n",
    "stemmed_results = apply_stemmers(documents)\n",
    "\n",
    "#print the result \n",
    "for original_doc, stems in stemmed_results.items():\n",
    "    print(f\"\\nOriginal Documnets: {original_doc}\" )\n",
    "    for stemmer_name,stemmed_words in stems.items():\n",
    "        print(f\"{stemmer_name.capitalize()},stems:{stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba8ae3",
   "metadata": {},
   "source": [
    "# but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4bf5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document : ['The', 'cats', 'are', 'running', 'and', 'jumping', 'in', 'the', 'garden', '.']\n",
      "Lemmatized:['The', 'cat', 'be', 'run', 'and', 'jump', 'in', 'the', 'garden', '.']\n",
      "========================================\n",
      "Original Document : ['She', 'is', 'a', 'beautiful', 'runner', 'and', 'loves', 'to', 'run', 'fast', '.']\n",
      "Lemmatized:['She', 'be', 'a', 'beautiful', 'runner', 'and', 'love', 'to', 'run', 'fast', '.']\n",
      "========================================\n",
      "Original Document : ['Running', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "Lemmatized:['Running', 'help', 'to', 'build', 'stamina', 'and', 'strength', '.']\n",
      "========================================\n",
      "Original Document : ['He', 'ran', 'swiftyly', 'and', 'caught', 'the', 'ball']\n",
      "Lemmatized:['He', 'run', 'swiftyly', 'and', 'catch', 'the', 'ball']\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initialze the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#function to apply lemmatization \n",
    "def apply_lemmatization(doc):\n",
    "    \n",
    "    #tokenize the documents\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    #apply lemmatization to each token \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token,pos='v') for token in tokens]\n",
    "    \n",
    "    return {\n",
    "        \"original\" : tokens,\n",
    "        \"lemmatized\":lemmatized_tokens\n",
    "    }\n",
    "    \n",
    "    \n",
    "#process each document and print result \n",
    "for doc in documents:\n",
    "    results = apply_lemmatization(doc)\n",
    "    print(f\"Original Document : {results['original']}\")\n",
    "    print(f\"Lemmatized:{results['lemmatized']}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "          \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da348060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cb745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
